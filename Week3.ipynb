{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOySO9zVQjLjpblJB3nqpyd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anusha-tikarya/Hexa_Project/blob/Week3/Week3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install and Set Up PySpark in Colab\n",
        "!pip install pyspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3ojJ_kDkwbw",
        "outputId": "3a6e09ea-dee1-40ef-93af-c6ae2777a435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840625 sha256=4cfcdbc494682b2d0c4f327fdccd091945209eee1f0e05c2e0e60abad2239191\n",
            "  Stored in directory: /root/.cache/pip/wheels/1b/3a/92/28b93e2fbfdbb07509ca4d6f50c5e407f48dce4ddbda69a4ab\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, count\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType\n"
      ],
      "metadata": {
        "id": "wIghdS8Uk0nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulate and Load Real-Time Data\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize the Spark session\n",
        "spark = SparkSession.builder.appName(\"EcommerceRealTimeProcessing\").getOrCreate()\n",
        "\n",
        "# Define schema for order data (order_id, product_id, customer_id, quantity, order_amount, order_date)\n",
        "schema = StructType([\n",
        "    StructField(\"order_id\", IntegerType(), True),\n",
        "    StructField(\"product_id\", IntegerType(), True),\n",
        "    StructField(\"customer_id\", IntegerType(), True),\n",
        "    StructField(\"quantity\", IntegerType(), True),\n",
        "    StructField(\"order_amount\", FloatType(), True),\n",
        "    StructField(\"order_date\", TimestampType(), True)\n",
        "])\n",
        "\n",
        "# Simulate real-time data using a list with datetime objects\n",
        "order_data = [\n",
        "    (1001, 1, 2001, 2, 139.99, datetime(2023, 9, 27, 12, 30, 0)),\n",
        "    (1002, 2, 2002, 1, 934.99, datetime(2023, 9, 27, 13, 45, 0)),\n",
        "    (1003, 3, 2003, 1, 543.99, datetime(2023, 9, 27, 14, 15, 0)),\n",
        "    (1004, 1, 2001, 3, 419.97, datetime(2023, 9, 27, 15, 0, 0)),\n",
        "    (1005, 2, 2003, 2, 1869.98, datetime(2023, 9, 27, 16, 5, 0)),\n",
        "]\n",
        "\n",
        "# Create a DataFrame from the list of orders\n",
        "df_orders = spark.createDataFrame(order_data, schema)\n",
        "\n",
        "# Show the data\n",
        "df_orders.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg1F7979lsTT",
        "outputId": "d72257d6-2897-4baf-c71a-b3dfbeb9eaac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+-----------+--------+------------+-------------------+\n",
            "|order_id|product_id|customer_id|quantity|order_amount|         order_date|\n",
            "+--------+----------+-----------+--------+------------+-------------------+\n",
            "|    1001|         1|       2001|       2|      139.99|2023-09-27 12:30:00|\n",
            "|    1002|         2|       2002|       1|      934.99|2023-09-27 13:45:00|\n",
            "|    1003|         3|       2003|       1|      543.99|2023-09-27 14:15:00|\n",
            "|    1004|         1|       2001|       3|      419.97|2023-09-27 15:00:00|\n",
            "|    1005|         2|       2003|       2|     1869.98|2023-09-27 16:05:00|\n",
            "+--------+----------+-----------+--------+------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Process Data Using PySpark\n",
        "# Group the data by product_id and calculate the total sales (sum of order_amount) per product\n",
        "product_sales = df_orders.groupBy(\"product_id\").agg(\n",
        "    sum(\"order_amount\").alias(\"total_sales\"),\n",
        "    count(\"order_id\").alias(\"order_count\")\n",
        ")\n",
        "\n",
        "# Show the result\n",
        "product_sales.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QbKtW_XmlHM",
        "outputId": "5d259e26-face-4e96-e533-270cac9539a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+-----------+\n",
            "|product_id|       total_sales|order_count|\n",
            "+----------+------------------+-----------+\n",
            "|         1|1119.9200286865234|          3|\n",
            "|         2| 2804.969970703125|          2|\n",
            "|         3|  1087.97998046875|          2|\n",
            "+----------+------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Real-Time Streaming Simulation\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "# Simulating appending new data in real-time\n",
        "new_order_data = [\n",
        "    (1006, 3, 2002, 1, 543.99, datetime(2023, 9, 27, 17, 0, 0)),\n",
        "    (1007, 1, 2001, 4, 559.96, datetime(2023, 9, 27, 17, 30, 0)),\n",
        "]\n",
        "\n",
        "# Create a new DataFrame for the new batch of orders with the correct timestamp format\n",
        "new_df_orders = spark.createDataFrame(new_order_data, schema)\n",
        "\n",
        "# Append the new data to the original DataFrame\n",
        "df_orders = df_orders.union(new_df_orders)\n",
        "\n",
        "# Perform the same aggregation again with the updated data\n",
        "updated_product_sales = df_orders.groupBy(\"product_id\").agg(\n",
        "    sum(\"order_amount\").alias(\"total_sales\"),\n",
        "    count(\"order_id\").alias(\"order_count\")\n",
        ")\n",
        "\n",
        "# Show the updated result\n",
        "updated_product_sales.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5egmv43l_4d",
        "outputId": "654c9228-7f00-4787-b3a4-1ad814767f65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+-----------+\n",
            "|product_id|       total_sales|order_count|\n",
            "+----------+------------------+-----------+\n",
            "|         1|1119.9200286865234|          3|\n",
            "|         2| 2804.969970703125|          2|\n",
            "|         3|  1087.97998046875|          2|\n",
            "+----------+------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the results to a CSV file\n",
        "updated_product_sales.write.csv(\"/content/product_sales.csv\", header=True)\n",
        "\n",
        "# Or, write to the console (in real-time streaming, you'd use .writeStream)\n",
        "updated_product_sales.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3BourS4mG44",
        "outputId": "da87b739-ae24-473d-88a3-d634b224b69f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+-----------+\n",
            "|product_id|       total_sales|order_count|\n",
            "+----------+------------------+-----------+\n",
            "|         1|1119.9200286865234|          3|\n",
            "|         2| 2804.969970703125|          2|\n",
            "|         3|  1087.97998046875|          2|\n",
            "+----------+------------------+-----------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}